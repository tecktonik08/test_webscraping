{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a824ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import datetime\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./db.sqlite3')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b76a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('./chromedriver.exe')\n",
    "browser.get('https://www.jobindexworld.com/jobpost/list')\n",
    "html = browser.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    target_name = 'jobindex'\n",
    "    target_url = 'https://www.jobindexworld.com/index'\n",
    "    detail_target_url = None\n",
    "    category_big = 'IT'\n",
    "    category_middle = None\n",
    "    category_small = None\n",
    "    search_word = None\n",
    "    \n",
    "    try:\n",
    "        now = datetime.datetime.now()\n",
    "        create_date = now.strftime(\"%Y%m%d\") # 스크래핑 날짜\n",
    "\n",
    "        # click해서 들어간 창\n",
    "        browser.find_elements_by_css_selector('article.cpn-circle-conts-item')[i].click() # 클릭 페이지\n",
    "        time.sleep(1)\n",
    "        html_scraping = browser.page_source \n",
    "        soup_scraping = BeautifulSoup(html_scraping, 'html.parser')\n",
    "\n",
    "        tags_signs = soup_scraping.select('div.circle-conts-view-main-conts-box-question > div') # '◈' 기호와 if문을 통해 form을 선별\n",
    "\n",
    "        recruit = soup_scraping.select('article.circle-conts-view > h3.tit-h3') # 주제, 제목\n",
    "        recruit_title = recruit[0].text.strip()\n",
    "\n",
    "        uri = soup_scraping.select('meta[property=\"og:url\"]') # 구직 회사 내용 화면\n",
    "        detail_uri = uri[0]['content']\n",
    "\n",
    "        start_date = soup_scraping.select('div.date')\n",
    "        start_date_t = start_date[0].text.strip()[1:9] # 날짜, 시간 -> data time: https://docs.python.org/ko/3/library/datetime.html\n",
    "        apply_start_date = '20'+start_date_t.replace('.','')\n",
    "        \n",
    "        \n",
    "    #---------------------------------------------------------------------------------------------------            \n",
    "        # 윗태그 2개 아래태그 5개 스크래핑 양식(career_requirements 컬럼이 없으므로 ' ' 추가)\n",
    "        if len(tags_signs) > 9:\n",
    "            tags_li = soup_scraping.select('div.circle-conts-view-main-conts-box-question > div > ul > li')\n",
    "            tags_li_data = []\n",
    "\n",
    "            for a in range(len(tags_li)):\n",
    "                tags_li_data.append(tags_li[a].text.strip().replace(' ','').replace('\\n','').split(':'))\n",
    "\n",
    "            for b in range(len(tags_li_data)):\n",
    "                if tags_li_data[b][0] == '회사개요':\n",
    "                    company_name = tags_li_data[b][1]\n",
    "                elif tags_li_data[b][0] == '모집기간':\n",
    "                    apply_end_date = tags_li_data[b][1]\n",
    "            \n",
    "\n",
    "            tags = soup_scraping.select('div.circle-conts-view-main-conts-box-question > div')\n",
    "            tags = tags[1:len(tags)]\n",
    "            tags_data = []\n",
    "\n",
    "            for c in range(len(tags)):\n",
    "                tags_data.append(tags[c].text.strip().replace('◈ ',''))\n",
    "\n",
    "            for d in range(len(tags_data)):\n",
    "                if tags_data[d] == '직무 개요':\n",
    "                    task = tags_data[d+1]\n",
    "\n",
    "                elif tags_data[d] == '경력 요건':\n",
    "                    need_career = tags_data[d+1]\n",
    "\n",
    "                elif tags_data[d] == '처우':\n",
    "                    team_env = tags_data[d+1]\n",
    "\n",
    "                elif tags_data[d] == '제출 서류 및 기타':\n",
    "                    need_doc = tags_data[d+1]\n",
    "\n",
    "            career_requirements = None\n",
    "            print(i,'페이지' ,'1번 스크래핑 양식')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "        # 윗태그 5개 스크래핑 양식(task, need_doc이 없으므로 ' '을 추가)\n",
    "        else:\n",
    "            tags_li = soup_scraping.select('div.circle-conts-view-main-conts-box-question > div > ul > li')\n",
    "            tags_li_data = []\n",
    "\n",
    "            for e in range(len(tags_li)):\n",
    "                tags_li_data.append(tags_li[e].text.strip().replace(' ','').replace('\\n','').split(':'))\n",
    "\n",
    "            for f in range(len(tags_li_data)):\n",
    "                if tags_li_data[f][0] == '회사개요':\n",
    "                    company_name = tags_li_data[f][1]\n",
    "\n",
    "                elif tags_li_data[f][0] == '경력레벨':\n",
    "                    need_career = tags_li_data[f][1]\n",
    "\n",
    "                elif tags_li_data[f][0] == '급여수준':\n",
    "                    team_env = tags_li_data[f][1]\n",
    "\n",
    "                elif tags_li_data[f][0] == '교육수준':\n",
    "                    career_requirements = tags_li_data[f][1]\n",
    "\n",
    "                elif tags_li_data[f][0] == '모집기간':\n",
    "                    apply_end_date = tags_li_data[f][1]\n",
    "\n",
    "\n",
    "            task = None\n",
    "            need_doc = None\n",
    "            print(i,'페이지','2번 스크래핑 양식')\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------       \n",
    "\n",
    "        contact = soup_scraping.select('span.writer-info') # 담당컨설턴트\n",
    "        contact_people = contact[0].text.strip()[0:3] # 이름 뽑아내기\n",
    "\n",
    "        hash_list = []\n",
    "        hash = soup_scraping.select('div.keyword-wrap > a.keyword') # 해시태그\n",
    "        for j in range(len(hash)):  # 반복문으로 해시태그들을 뽑아 리스트로 만들기\n",
    "            hash_t = hash[j].text.strip()\n",
    "            hash_list.append(hash_t)\n",
    "        \n",
    "        hashtag = ''\n",
    "        for k in hash_list:\n",
    "            hashtag = hashtag + k\n",
    "            \n",
    "        print(id, target_name, target_url, detail_target_url, category_big, category_middle, category_small, search_word, create_date, recruit_title, company_name, detail_uri, apply_start_date, apply_end_date, task, need_career, team_env, need_doc, contact_people, hashtag)    \n",
    "        time.sleep(1)\n",
    "        cur.execute('''insert into SCRAPPING_SITE_B(\n",
    "            apply_end_date,\n",
    "            apply_start_date,\n",
    "            career_requirements,\n",
    "            category_big,\n",
    "            company_name,\n",
    "            contact_people,\n",
    "            detail_uri,\n",
    "            hashtag,\n",
    "            need_career,\n",
    "            need_doc,\n",
    "            recruit_title,\n",
    "            target_name,\n",
    "            target_url,\n",
    "            task,\n",
    "            team_env)\n",
    "        values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', (apply_end_date, apply_start_date, career_requirements, category_big, company_name, contact_people, detail_uri, hashtag, need_career, need_doc, recruit_title, target_name, target_url, task, team_env))\n",
    "        conn.commit()\n",
    "        browser.back()\n",
    "        # 더보기 button 클릭하기 태그 : i.icon-plus-sign-to-add\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT * FROM SCRAPPING_SITE_B\") # newspaper text(기사)의 내용을 추출\n",
    "\n",
    "articles = [] # list 선언\n",
    "\n",
    "for row in cur.fetchall():\n",
    "  articles.append(row) \n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dc642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
